\section[EVALUACIÓN DE UN SISTEMA DE RECOMENDACIÓN]{PARADIGMAS Y MÉTRICAS DE EVALUACIÓN PARA UN SISTEMA DE RECOMENDACIÓN: }

Los Sistemas de Recomendación deben desarrollarse mediante un enfoque alineado con los objetivos esperados respecto a la interacción con los usuarios. Estos objetivos principales han sido estudiados y seccionados en paradigmas de evaluación, sin embargo, basándose en la investigación de \parencite{Aggarwal2016} se pueden definir también métricas de evaluación generales para calificar el desempeño de un sistema de recomendación. En esta sección se mencionarán generalidades de los paradigmas de evaluación y se hará un enfasis en las métricas generales.

\subsection{PARADIGMAS DE EVALUACIÓN: }

Existen tres paradigmas de evaluación principales dirigidos a los sistemas de recomendación, donde las principales diferencias se centran en cómo los usuarios interactúan con las métricas de evaluación.

\begin{enumerate}

    \item \textbf{Usuarios de Estudio: } Este paradigma se centra en invitar a los usuarios a un proceso de evaluación para interactuar con el sistema y evaluar su desempeño en contextos específicos. Por lo tanto, las mediciones se obtienen de los comentarios de los usuarios y de las observaciones de interacción obtenidas  por el sistema. 
    Esta información se usa para entender cuáles fueron las preferencias del usuario, qué le gustó y qué no le gustó.

    La principal ventaja de este paradigma es que los usuarios dan permisos explícitos para usar su información en función de la evaluación del sistema, sin embargo, las desventajas más significativas radican en los cambios inconscientes del usuario al estar en un escenario de evaluación y en la poca escalabilidad del método. 
    Es probable que no todos los usuarios estén de acuerdo en participar en un escenario de evaluación, y aquellos que lo hagan podrían no tener preferencias significativas alineadas con la mayoría de la población o, en resumen, se podrían obtener resultados específicos pero no representativos.

    Las desventajas mencionadas ocasionan que este paradigma sea usado sólo en entornos académicos o de proyectos que no sean de gran escala.
    
    \item{\textbf{Evaluación \textit{Online}}}: Este paradigma de igual manera depende de la interacción del usuario pero en un entorno de uso real, lo que fomenta resultados más naturales debido a que ahora se interactúa con el sistema en una situación cotidiana. Una de las fortalezas más importantes de este paradigma es que funciona combinando diferentes \textit{enfoques de los sistemas de recomendación} para evaluar cuál se comporta mejor en un escenario en particular. 
    Para comparar el rendimiento de diferentes enfoques se usan métricas concretas como la tasa de conversión, CTR, tiempo de interacción, etc... Estos métodos son también conocidos como \textit{Pruebas} $A / B$ y miden el impacto directo del sistema de recomendación hacia el usuario. 
 
    \textit{"La idea básica es similar a un apostador (el sistema de recomendación) que debe elegir una máquina tragamonedas (diferentes enfoques de recomendación) en un casino. El apostador sospecha que una de las tragamonedas tiene un porcentaje de retorno más alto (tasa de conversión, CTR, etc...) que las demás. Por lo tanto,  el apostador juega con cada tragamonedas de forma individual para observar el porcentaje de retorno de cada máquina. De forma codiciosa el apostador selecciona solo aquella tragamonedas que tenga un rendimiento mas efectivo respecto a las demás"} \parencite{Aggarwal2016}.

    Una de las ventajas más grandes de este paradigma es que permite medir de forma algorítmica el rendimiendo de diferentes \textit{enfoques de recomendación} para obtener el mejor rendimiento posible a través de la interacción directa del usuario. Sin embargo, la principal desventaja es que estos sistemas no pueden ser implementados de forma realista a menos de que se tenga una cantidad masiva de usuarios, haciendo que sea muy complejo de usar en las fases iniciales del sistema.

    La necesidad de contar con una cantidad masiva de usuarios surge de los métodos de \textit{Pruebas} $A / B$ que agrupa a los usuarios en muestras aleatorias a las que se les aplican diferentes algoritmos de recomendación y se mide la satisfacción del usuario mediante diferentes métricas. Para que las mediciones retornen resultados relevantes cada grupo debe contar con un gran número de usuarios.

    \item \textbf{Evaluación \textit{Offline} con \textit{Datasets}\footnote{\textbf{Dataset: } Es una colección de elementos relacionados organizados y con un formato para cumplir un propósito particular \parencite{chapman-2019}.} Históricos: } En la evaluación Offline se trabaja con datos históricos como, por ejemplo, las calificaciones de un usuario a items del sistema. La principal ventaja de trabajar con \textit{Datasets} de información es que se tiene un conjunto grande de información previamente recopilada y no se tiene la necesidad realizar un seguimiento a miles de usuarios activos para evaluar el sistema. Una vez que un \textit{Dataset} ha sido recolectado se puede usar como principal punto de referencia para comparar diferentes algoritmos.
    
    Además, se pueden utilizar diversos datasets que favorezcan la \textit{generalidad} del Sistema de Recomendación.

    Sin embargo, la principal desventaja de la \textit{Evaluación Offline} es que no logra predecir cómo reaccionará el usuario al sistema en el futuro.
    A pesar de estas áreas de oportunidad, éste paradigma sigue siendo uno de los más usados y aceptados para la evaluación de sistemas de recomendación, esto se debe a que las estadísticas que se pueden obtener de estos métodos son robustas, confiables y fáciles de entender.

\end{enumerate}

\subsection{MÉTRICAS GENERALES DE EVALUACIÓN: }

Además de los paradigmas de evaluación que definen metodologías específicas para realizar pruebas a los usuarios, existen métricas cuantificables que añaden una capa extra de complejidad y robustez a los sistemas de recomendación. Estas metricas cumplen la función de asegurar resultados satisfactorios para el usuario mediante características cuantificables y subjetivas, entendiendo que el gusto y las preferencias pueden ser complejas de representar mediante mediciones numéricas.

    \subsubsection{EXACTITUD}

    La exactitud es una de las métricas fundamentales con las que se evalúan los sistemas de recomendación. Se basan en \textit{ratings} que se definen como \textit{"cantidades numéricas que deben ser estimadas" } \parencite{Aggarwal2016}. Sin embargo, como ha sido mencionado anteriormente, algunos enfoques de recomendación no se centran en predecir calificaciones sino que reunen el top-$k$ de los \textit{items} más favorables. 

    \newpage

    La medición por exactitud tiene diversos componentes que aseguran su efectividad:

    \begin{itemize}
        \item \textbf{\textit{Métricas de Efectividad: }} Las Métricas de Efectividad son usadas para evaluar tanto la efectividad de predicción para estimar la calificación de un usuario específico respecto a un item o la efectividad del top - $k$ sugerido por el sistema de recomendación \parencite{Aggarwal2016}.
        
        Existen dos métodos diferentes para medir la efectividad de las predicciones y de los top - $k$:

        \begin{itemize}[label=$\diamond$]
            \item \textbf{\emph{Efectividad de Predicción: }} Se centra en medir el \textit{Error Específico de Entrada} \footnote{\textbf{Error Específico de Entrada: } Diferencia entre la calificación que el sistema de recomendación predice y la calificación real que el usuario dio.} que se define mediante la siguiente fórmula:
            \begin{equation}
                \Large e_{uj} \ = \ \hat{r}_{uj} \ - \ r_{uj}
            \addequation{Definición de Error Específico de Entrada}
            \end{equation}

            \begin{description}
                \item[\textbf{Donde: }] 
                \item[$u$] representa al usuario.
                \item[$j$] representa al item.
                \item[$\hat{r}_{uj}$] representa la predicción de calificación.
                \item[$r_{uj}$] representa la calificación real del usuario. 
            \end{description}

            Este cálculo de $e_{uj}$ puede ser aprovechado de diferentes maneras para hacer un calculo sobre el conjunto de errores de predicción $E$ y los rankings vinculados a cada $e \in E$ \footnote{\textbf{Pertenencia $\in$}: Éste símbolo indica que un elemento cualquiera $u$ \textit{pertenece} a un conjunto $R$, por lo tanto $u \in R$}.

            Para sintetizar, se dice que el conjunto $E$ es definido por: 

            \begin{equation}
                E \ =  \{ \  e_{uj} \ |  \ (u,j) \in D \ \}
                \addequation{Definición del Conjunto de Entradas $E$}
            \end{equation}
            
            Recordando que $e_{uj}$ representa el \textit{Error específico de Entrada} del usuario $u$ para el item $j$. La fórmula indica que a cada par $( u, \ j )$ perteneciente a todos los posibles pares en el conjunto $D$ (o, en otras palabras, el conjunto de items evaluados por el usuario) tiene asignado un valor $e_{uj}$. 

            Este valor de \textit{Error específico de entrada} puede indicar la calidad de un sistema de recomendación si se usa en conjunto con métricas de efectividad.

            \newpage

            \textit{\textbf{Mean Squared Error (MSE): }} 
            
            El Error Cuadrático Medio es una métrica utilizada para evaluar la diferencia entre las calificaciones predichas por un modelo y las calificaciones reales del usuario. Funciona calculando el promedio de los errores elevados al cuadrado. Un valor MSE más bajo indica que el modelo tiene un mejor rendimiento, ya que sus predicciones están más cerca de los valores reales.
            
            La forma del MSE es: 

            \begin{equation}
                MSE = \frac{\sum_{(u, j) \ \in \ E \ }{e^2_{uj}}}{| \ E \ |}
                \addequation{Definición de Mean Squared Error}
            \end{equation}

            Que se puede resumir en la sumatoria de todos los $e_{uj}^2 \in E$ divididos entre la cantidad de elementos en $E$ para obtener un promedio, haciendo así que un valor alto de $e_{uj}$, al ser elevado al cuadrado, penalice de forma drástica los errores del sistema.

            \textbf{\textit{Root Mean Squared Error (RMSE): }} Como su nombre lo índica, esta métrica se basa en el MSE con la ligera diferencia de aplicar raíz cuadrada de la siguiente manera: 

            \begin{equation}
                RMSE = \sqrt{\frac{\sum_{(u,j) \ \in \ E}{\ e^2_{uj}}}{ \ |E| \ }}
                \addequation{Definición de Root Mean Squared Error}
            \end{equation}
            
            En ésta definición se puede observar que es similar a MSE, la única diferencia radica en la escala de los resultados. RMSE mantiene las mismas unidades que la variable original y suele representar el formato estándar de error \parencite{gmd-15-5481-2022}.

            \item \textbf{Efectividad de Ranking: } Para los sistemas enfocados en \textit{rankings} se pueden usar mediciones diferentes, algunas de ellas son las siguientes.
            
            \textbf{\textit{Evaluando Rankings vía Correlación: }}

            Cuando un sistema de recomendación retorna un top - $k$ de recomendaciones, en general, es deseable que los items más altos en el top sean más deseados por el usuario que los items más bajos.
            Por lo tanto, la idea central de la \textit{Evaluación de Rankings vía Correlación} se centra en identificar qué tan correcto fue el desempeño del ranking retornado por el sistema de recomendación en relación con el ranking decidido por el usuario de forma manual.
            
            \newpage

            Si se quisiera definir formalmente, se podría decir que ésta evaluación se centra en evaluar la similitud entre:

            \begin{equation}
                \begin{split}
                    R_j &= \{ i_{j1}, i_{j2}, i_{j3}, ..., i_{jn} \}\\
                    R_u &= \{ i_{u1}, i_{u2}, i_{u3}, ..., i_{un} \}
                \end{split}
                \addequation{Definición de Rankings de Usuario y de Sistema}
            \end{equation}  

            \begin{description}
                \item[\textbf{Donde:}]
                \item[$j$] representa al sistema de recomendación.
                \item[$u$] representa al usuario.
                \item[$R_j$] representa el \textit{Ranking} recomendado por el sistema.
                \item[$R_u$] representa el \textit{Ranking} elegido por el usuario.
                \item[$i$]  representa un item dentro de un \textit{Ranking}.      
            \end{description}

            Se debe tomar en cuenta que el análisis de correlación entre rankings no solo toma en cuenta que $i \in R$ tomando en cuenta ambos $R_u$ y $R_j$, sino que también toma en cuenta el \textit{orden} en que se encuentran los items.
            
            Un detalle importante a tener encuenta es que las \textit{calificaciones} suelen elegirse en una escala discreta, y podrían existir muchos empates en la verdad del mundo real (\textit{ground truth}) \parencite{Aggarwal2016}.
            Por lo tanto, tomando en cuenta que un usuario elige una calificación dentro de un conjunto discreto \footnote{\textbf{Conjuntos Discretos: } Un conjunto discreto es un conjunto formado por números en el cual entre cada número y el siguiente no hay ningún otro. \parencite{Perez_MAT_2021}}, por ejemplo, calificaciónes enteras del 1 al 5, o calificaciones basadas en estrellas, es probable que las valoraciones del usuario caigan en empates para diferentes items.
            Esta característica complica el cálculo de correlación, obligando al sistema a tener cuidado con la naturaleza poco fina de las calificaciones del usuario.
            
            Para obtener el valor de similitud se usan las  métricas \textit{Spearman Rank Correlation}, \textit{Kendall Rank Correlation} para obtener una medición exacta de la similitud entre $R_u$ y $R_j$. 
        \end{itemize}         
    \end{itemize}
    
    \newpage
    
    \subsubsection{COBERTURA}

    Como se ha mencionado anteriormente, los sistemas de recomendación podrían tener dificultades en encontrar resultados relevantes cuando no se tiene la suficiente información de los \textit{items} y de los \textit{usuarios}, por ejemplo, cuando un usuario nuevo interactúa con la plataforma, aunque el sistema de recomendación tenga un excelente rendimiento en exactitud, es probable que no brinde resultados relevantes debido a la falta de información. 

    \textit{"Esta limitación de los sistemas de recomendación es consecuencia del hecho de que las matrices de calificación suelen ser escasas."} \parencite{Aggarwal2016}

    Para medir qué tan propenso es el sistema a caer en esta dificultad se usa el concepto de \textit{Cobertura}, que mide la proporción de items que el sistema puede recomendar a un usuario.
    Sin embargo, generalmente durante la evaluación de sistemas de recomendación, es común que favorecer la \textit{exactitud} tenga como consecuencia reducir la \textit{cobertura} debido a que la \textit{exactitud} favorece los items más parecidos a las preferencias del usuario, y la \textit{cobertura} se centra en brindar diversidad de items. Es por esto que, en general, la mejor decisión sea obtener un balance entre ambas métricas sin buscar maximizar alguna de las dos.

    \subsubsection{CONFIABILIDAD Y CERTEZA }

    La estimación de calificaciones es un proceso inexacto que depende directamente de ciertos algoritmos que pueden proveer diferentes predicciones. Por lo tanto, es común que el usuario pierda confianza en el sistema al no saber qué tan acertadas serán las recomendaciones.

    Para medir esta dificultad se usan métricas de confiabilidad que tratan de medir qué tan confiable es la recomendación que el sistema ha brindado.

    \textit{"En general, los sistemas de recomendación que pueden recomendar de forma acertada elementos con un menor intervalo de confianza son más deseables debido a que refuerzan la confiabilidad del usuario por el sistema".} \parencite{Aggarwal2016}

    Por ejemplo, si el sistema obtiene un intervalo de confiabilidad de 4.8 a 5.0 significa que tiene un rendimiento certero debido a que la distancia entre intervalos es pequeña. \textit{"El método más simple para obtener intervalos de confianza es asumir que la cantidad de interés es tiene una distribución gaussiana  caracterizada por su forma de campana.}." \parencite{10.5555/1941884}

    \subsubsection{NOVEDAD}

    La \textit{Novedad} en un sistema de recomendación se define como la probabilidad de que el sistema sugiera un item que el usuario no ha visto antes.
    Generalmente, es más importante que un usuario brinde información al sistema sobre elementos totalmente nuevos, con el objetivo de brindar una nueva perspectiva de sus preferencias al sistema. Inclusive podría ser más importante la calificación del usuario respecto a items que no ha visto antes en comparación a calificar items faltantes que son fieles al gusto del usuario. 

    En algunos enfoques de recomendación como el \textit{basado en contenido} es común que las sugerencias sean excesivamente apegadas al gusto del usuario, causando que éste pierda confianza en el sistema y lo considere repetitivo.

    \textit{"La forma más natural de medir la novedad en un sistema es a través de experimentaciones Online en donde el usuario es cuestionado sobre su conocimiento previo de los items recomendados"} \parencite{Aggarwal2016}.

    Sin embargo, como se mencionó anteriormente en el paradigma \textit{Online}, éste suele ser poco escalable en sistemas grandes. Por lo tanto, generalmente es mejor opción combinar un enfoque \textit{Online} y \textit{Offline} para realizar las mediciones de novedad en un sistema.

    \subsubsection{SERENDIPIA}

    La \textit{Serendipia} es la medición del nivel de sorpresa de recomendaciones exitosas \parencite{Aggarwal2016}. 
    
    Las ventajas de la Serendipia respecto a la Novedad es que, además de recomendar items nuevos para el usuario, también busca dar recomendaciones \textit{no obvias} que lo sorprendan. Entonces, se puede decir que la serendipia se compone de relevancia, novedad y sorpresa.
    
    Según \parencite{Kotkov2020Serendipity}, una recomendación es \textit{Relevante} para el usuario si éste ha expresado preferencia (o lo hará) por el item. Dependiendo del escenario, el método de expresar preferencia podría variar. En adición, la \textit{Sorpresa} es lograda cuando el sistema recomienda un item que es inesperado para el usuario y, además, se estima que no lo habría descubierto por sí mismo.

    \newpage

    \subsubsection{DIVERSIDAD}

    La \textit{Diversidad} implica que el conjunto de recomendaciones debe ser suficientemente diverso para que, si al usuario no le gusta el $item_i$, la probabilidad de que le guste otro item dentro del conjunto siga siendo alta, logrando que el conjunto no esté compuesto únicamente de items del mismo tipo y evitando la fatiga del usuario. 
    
    Debido a esto la \textit{Diversidad} está estrechamente relacionada con la \textit{Serendipia} y la \textit{Novedad}, ya que un conjunto ampliamente diverso incrementa el porcentaje de items nuevos e inesperados para el usuario.
    
    Este aspecto se suele medir usando una representación vectorial del conjunto de recomendaciones (este concepto se explicará más adelante), definiendo así a la \textit{Diversidad} como el \textbf{promedio} de la similitud entre cada par de items (en su representación vectorial). Si el valor del promedio es pequeño se considera una mayor diversidad.







