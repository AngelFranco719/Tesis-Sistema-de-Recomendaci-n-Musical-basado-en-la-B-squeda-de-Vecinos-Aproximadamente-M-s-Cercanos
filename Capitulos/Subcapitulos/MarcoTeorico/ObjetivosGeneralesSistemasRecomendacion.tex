\section{PARADIGMAS Y MÉTRICAS DE EVALUACIÓN PARA UN SISTEMA DE RECOMENDACIÓN: }

Los Sistemas de Recomendación deben desarrollarse mediante un enfoque alineado con los objetivos esperados respecto a la interacción con los usuarios. Estos objetivos principales han sido estudiados y seccionados en paradigmas de evaluación, sin embargo, basándose en la investigación de \parencite{Aggarwal2016} se pueden definir también métricas de evaluación generales para calificar el desempeño de un sistema de recomendación. En esta sección se mencionarán generalidades de los paradigmas de evaluación y se hará un enfasis en las métricas generales.

\subsection{PARADIGMAS DE EVALUACIÓN: }

Existen tres paradigmas de evaluación principales dirigidos a los sistemas de recomendación, donde las principales diferencias se centran en cómo los usuarios interactúan con las métricas de evaluación.

\begin{enumerate}

    \item \textbf{Usuarios de Estudio: } Este paradigma se centra en invitar a los usuarios a un proceso de evaluación para interactuar con el sistema y evaluar su desempeño en contextos específicos. Por lo tanto, las mediciones se obtienen de los comentarios de los usuarios y de las observaciones de interacción obtenidas  por el sistema. 
    Esta información se usa para entender cuáles fueron las preferencias del usuario, qué le gustó y qué no le gustó.

    La principal ventaja de este paradigma es que los usuarios dan permisos explícitos para usar su información en función de la evaluación del sistema, sin embargo, las desventajas más significativas radican en los cambios inconscientes del usuario al estar en un escenario de evaluación y en la poca escalabilidad del método. 
    Es probable que no todos los usuarios estén de acuerdo en participar en un escenario de evaluación, y aquellos que lo hagan podrían no tener preferencias significativas alineadas con la mayoría de la población o, en resumen, se podrían obtener resultados específicos pero no representativos.

    Las desventajas mencionadas ocasionan que este paradigma sea usado sólo en entornos académicos o de proyectos que no sean de gran escala.

    \newpage
    \thispagestyle{plain}
    \vspace*{0.2cm}
    
    \item{\textbf{Evaluación \textit{Online}}}: Este paradigma de igual manera depende de la interacción del usuario pero en un entorno de uso real, lo que fomenta resultados más naturales debido a que ahora se interactúa con el sistema en una situación cotidiana. Una de las fortalezas más importantes de este paradigma es que funciona combinando diferentes \textit{enfoques de los sistemas de recomendación} para evaluar cuál se comporta mejor en un escenario en particular. 
    Para comparar el rendimiento de diferentes enfoques se usan métricas concretas como la tasa de conversión, CTR, tiempo de interacción, etc... Estos métodos son también conocidos como \textit{Pruebas} $A / B$ y miden el impacto directo del sistema de recomendación hacia el usuario. 
 
    \textit{"La idea básica es similar a un apostador (el sistema de recomendación) que debe elegir una máquina tragamonedas (diferentes enfoques de recomendación) en un casino. El apostador sospecha que una de las tragamonedas tiene un porcentaje de retorno más alto (tasa de conversión, CTR, etc...) que las demás. Por lo tanto,  el apostador juega con cada tragamonedas de forma individual para observar el porcentaje de retorno de cada máquina. De forma codiciosa el apostador selecciona solo aquella tragamonedas que tenga un rendimiento mas efectivo respecto a las demás"} \parencite{Aggarwal2016}.

    Una de las ventajas más grandes de este paradigma es que permite medir de forma algorítmica el rendimiendo de diferentes \textit{enfoques de recomendación} para obtener el mejor rendimiento posible a través de la interacción directa del usuario. Sin embargo, la principal desventaja es que estos sistemas no pueden ser implementados de forma realista a menos de que se tenga una cantidad masiva de usuarios, haciendo que sea muy complejo de usar en las fases iniciales del sistema.

    La necesidad de contar con una cantidad masiva de usuarios surge de los métodos de \textit{Pruebas} $A / B$ que agrupa a los usuarios en muestras aleatorias a las que se les aplican diferentes algoritmos de recomendación y se mide la satisfacción del usuario mediante diferentes métricas. Para que las mediciones retornen resultados relevantes cada grupo debe contar con un gran número de usuarios.

    \newpage
    \thispagestyle{plain}
    \vspace*{0.2cm}

    \item \textbf{Evaluación \textit{Offline} con \textit{Datasets}\footnote{\textbf{Dataset: } Es una colección de elementos relacionados organizados y con un formato para cumplir un propósito particular \parencite{chapman-2019}.} Históricos: } En la evaluación Offline se trabaja con datos históricos como, por ejemplo, las calificaciones de un usuario a items del sistema. La principal ventaja de trabajar con \textit{Datasets} de información es que se tiene un conjunto grande de información previamente recopilada y no se tiene la necesidad realizar un seguimiento a miles de usuarios activos para evaluar el sistema. Una vez que un \textit{Dataset} ha sido recolectado se puede usar como principal punto de referencia para comparar diferentes algoritmos.
    
    Además, se pueden utilizar diversos datasets que favorezcan la \textit{generalidad} del Sistema de Recomendación.

    Sin embargo, la principal desventaja de la \textit{Evaluación Offline} es que no logra predecir cómo reaccionará el usuario al sistema en el futuro.
    A pesar de estas áreas de oportunidad, éste paradigma sigue siendo uno de los más usados y aceptados para la evaluación de sistemas de recomendación, esto se debe a que las estadísticas que se pueden obtener de estos métodos son robustas, confiables y fáciles de entender.

\end{enumerate}

\newpage
\thispagestyle{plain}
\vspace*{0.2cm}

\subsection{MÉTRICAS GENERALES DE EVALUACIÓN: }

Además de los paradigmas de evaluación que definen metodologías específicas para realizar pruebas a los usuarios, existen métricas cuantificables que añaden una capa extra de complejidad y robustez a los sistemas de recomendación. Estas metricas cumplen la función de asegurar resultados satisfactorios para el usuario mediante características cuantificables y subjetivas, entendiendo que el gusto y las preferencias pueden ser complejas de representar mediante mediciones numéricas.

\begin{enumerate}
    \item \textbf{Exactitud: } La exactitud es una de las métricas fundamentales con las que se evalúan los sistemas de recomendación. Se basan en \textit{ratings} que se definen como \textit{"cantidades numéricas que deben ser estimadas" } \parencite{Aggarwal2016}. Sin embargo, como ha sido mencionado anteriormente, algunos enfoques de recomendación no se centran en predecir \textit{rankings} sino que reunen el top-$k$ de los \textit{items} más favorables. 

    La medición por exactitud tiene diversos componentes que aseguran su efectividad:

    \begin{itemize}
        \item \textbf{\textit{Métricas de Efectividad: }} Las Métricas de Efectividad son usadas para evaluar tanto la efectividad de predicción para estimar la calificación de un usuario específico respecto a un item o la efectividad del top - $k$ sugerido por el sistema de recomendación \parencite{Aggarwal2016}.
        
        Existen dos métodos diferentes para medir la efectividad de las predicciones y de los top - $k$:

        \begin{itemize}[label=$\diamond$]
            \item \textbf{\emph{Efectividad de Predicción: }} Se centra en medir el \textit{Error Específico de Entrada} \footnote{\textbf{Error Específico de Entrada: } Diferencia entre la calificación que el sistema de recomendación predice y la calificación real que el usuario dio.} que se define mediante la siguiente fórmula:
            \begin{Equation}
            \[
                e_{uj} \ = \ \hat{r}_{uj} \ - \ r_{uj}
            \]
            \caption{Error Específico de Entrada}
            \label{eq:Error específico de entrada}
            \addequation{Error Específico de Entrada}
            \end{Equation}

            \begin{description}
                \item[\textbf{Donde: }] 
                \item[$u$] representa al usuario.
                \item[$j$] representa al item.
                \item[$\hat{r}_{uj}$] representa la predicción de calificación.
                \item[$r_{uj}$] representa la calificación real del usuario. 
            \end{description}

        \end{itemize}
    \end{itemize}

    %\begin{Equation}
    %\[
    %a^2 + b^2 = c^2
    %\]
    %\caption{Teorema de Pitágoras}
    %\label{eq:pitagoras}
    %\addequation{Teorema de Pitágoras}
    %\end{Equation}

\end{enumerate}



